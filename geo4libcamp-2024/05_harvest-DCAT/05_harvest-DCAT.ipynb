{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a446268",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "This script will scan the [DCAT 1.1 API](https://resources.data.gov/resources/dcat-us/) of ArcGIS Hubs and return the metadata for all suitable items as a CSV file in the GeoBTAA Metadata Application Profile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7df424",
   "metadata": {},
   "source": [
    "## Review the list of active ArcGIS Hubs\n",
    "\n",
    "`arcHubs.csv` is a file containing metadata associated with each Hub that we will use for default values:\n",
    "\n",
    "* **ID**: Unique code assigned to each portal. This is transferred to the \"Is Part Of\" field for each dataset.\n",
    "* **Title**: The name of the Hub. This is transferred to the \"Provider\" field for each dataset\n",
    "* **Publisher**: The place or administration associated with the portal. This is applied to the title in each dataset in brackets\n",
    "* **Spatial Coverage**: A list of place names. These are transferred to the Spatial Coverage for each dataset\n",
    "* **Member Of**: a larger collection level record. This is custom for the BTAA Geoportal, where items from Hubs are assigned to either the [Government Open Geospatial Data Collection](https://geo.btaa.org/catalog/ba5cc745-21c5-4ae9-954b-72dd8db6815a) or the [Research Institutes Geospatial Data Collection](https://geo.btaa.org/catalog/b0153110-e455-4ced-9114-9b13250a7093)\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8a6b2",
   "metadata": {},
   "source": [
    "## Import Modules\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff38b476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv # Provides functionality to read from and write to CSV files.\n",
    "import json # Provides functionality to work with JSON data.\n",
    "import re # Provides regular expression matching operations.\n",
    "import time # Provides functions for working with time, including time conversion, sleep function and timers.\n",
    "import urllib.request # provides functions for working with URLs, like opening URLs, reading data from URLs, etc.\n",
    "from html.parser import HTMLParser # provides an HTML parsing library that can be used to extract data from HTML docs.\n",
    "from urllib.parse import urlparse, parse_qs # provides a way to parse URLs into their components.\n",
    "import pandas as pd # Provides data manipulation and analysis functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6274c4a",
   "metadata": {},
   "source": [
    "**Set up paths and output CSV field names**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a38c9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \".\"  # Set to directory containing arcHubs.csv\n",
    "hubFile = \"arcHubs.csv\"  # the name of the CSV file with the list of ArcGIS Hubs\n",
    "fieldnames = [  # DCAT schema fields to be included in report\n",
    "    \"Title\",\n",
    "    \"Alternative Title\",\n",
    "    \"Description\",\n",
    "    \"Language\",\n",
    "    \"Display Note\",\n",
    "    \"Creator\",\n",
    "    \"Title Source\",\n",
    "    \"Resource Class\",\n",
    "    \"Resource Type\",\n",
    "    \"Keyword\",\n",
    "    \"Date Issued\",\n",
    "    \"Temporal Coverage\",\n",
    "    \"Date Range\",\n",
    "    \"Spatial Coverage\",\n",
    "    \"Bounding Box\",\n",
    "    \"Format\",\n",
    "    \"Information\",\n",
    "    \"Download\",\n",
    "    \"MapServer\",\n",
    "    \"FeatureServer\",\n",
    "    \"ImageServer\",\n",
    "    \"ID\",\n",
    "    \"Identifier\",\n",
    "    \"Provider\",\n",
    "    \"Code\",\n",
    "    \"Member Of\",\n",
    "    \"Is Part Of\",\n",
    "    \"Rights\",\n",
    "    \"Accrual Method\",\n",
    "    \"Date Accessioned\",\n",
    "    \"Access Rights\",\n",
    "]\n",
    "\n",
    "ActionDate = time.strftime('%Y%m%d') # Generate the current local time with the format like 'YYYYMMDD' and save to the variable named 'ActionDate'\n",
    "\n",
    "json_ids = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ba35c",
   "metadata": {},
   "source": [
    "**Function to remove HTML tags**\n",
    "\n",
    "Sometimes, the metadata fields we scrape contain HTML tags, such as links or formatting that do not work in the Geoportal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7616ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLStripper(HTMLParser): \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.fed = []\n",
    "\n",
    "    def handle_data(self, d): \n",
    "        self.fed.append(d)\n",
    "\n",
    "    def get_data(self): # Returns a string of all the data in the list concatenated together.\n",
    "        return \"\".join(self.fed)\n",
    "\n",
    "\n",
    "def strip_tags(html): # Defined by the MLS Stripper \n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "\n",
    "def cleanData(value): # Calls strip_tags on the input value to remove any HTML tags present.\n",
    "    return strip_tags(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220f9c9",
   "metadata": {},
   "source": [
    "**Function to generate an output CSV**\n",
    "\n",
    "iterate over the keys and writes the corresponding values to the CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6391c727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printItemReport(report, fields, dictionary):\n",
    "    with open(report, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        csvout = csv.writer(outfile)\n",
    "        csvout.writerow(fields)\n",
    "        for hub in dictionary:\n",
    "            for keys in hub:\n",
    "                allvalues = hub[keys]\n",
    "                csvout.writerow(allvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba373d6",
   "metadata": {},
   "source": [
    "**Function to create a dictionary of metadata in the JSONs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f3391b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the len function to get the number of datasets and the range function to loop through each dataset\n",
    "        \n",
    "def getIdentifiers(data):\n",
    "    json_ids = {}  # Dictionary List\n",
    "    for x in range(len(data[\"dataset\"])):\n",
    "        json_ids[x] = data[\"dataset\"][x][\"identifier\"]\n",
    "    return json_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992a3aea",
   "metadata": {},
   "source": [
    "**Function to generate the title as: alternativeTitle [place name] {year}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1908624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function uses regular expressions to extract the year from the alternative title, and replaces it with an empty string to remove it from the title.\n",
    "\n",
    "def format_title(alternativeTitle, titleSource):\n",
    "    # find if year exist in alternativeTitle\n",
    "    year = ''\n",
    "    try:  \n",
    "      year_range = re.findall(r'(\\d{4})-(\\d{4})', alternativeTitle)\n",
    "    except:\n",
    "      year_range = ''\n",
    "    try: \n",
    "      single_year = re.match(r'.*(17\\d{2}|18\\d{2}|19\\d{2}|20\\d{2})', alternativeTitle)\n",
    "    except:\n",
    "      single_year = ''    \n",
    "    if year_range:   # if a 'yyyy-yyyy' exists\n",
    "        year = '-'.join(year_range[0])\n",
    "        alternativeTitle = alternativeTitle.replace(year, '').strip().rstrip(',')\n",
    "    elif single_year:  # or if a 'yyyy' exists\n",
    "        year = single_year.group(1)\n",
    "        alternativeTitle = alternativeTitle.replace(year, '').strip().rstrip(',')\n",
    "     \n",
    "    altTitle = str(alternativeTitle)\n",
    "    title = altTitle + ' [{}]'.format(titleSource)   \n",
    "    if year:\n",
    "        title += ' {' + year +'}'       \n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a9df9",
   "metadata": {},
   "source": [
    "**Function to create a dictionary of scanned metadata**\n",
    "\n",
    "This code defines a function called `metadataNewItems()` which takes two arguments \n",
    "\n",
    "* `newdata` (a dictionary containing metadata information about new items)\n",
    "* `newitem_ids` (a dictionary containing information about the new items such as the position and the landing page URLs).\n",
    "\n",
    "The function processes the metadata information for each new item and creates a dictionary containing the formatted metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eb10c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This includes blank fields '' for some columns\n",
    "\n",
    "def metadataNewItems(newdata, newitem_ids):\n",
    "    newItemDict = {}\n",
    "    # y = position of the dataset in the DCAT metadata json, v = landing page URLs\n",
    "    for y, v in newitem_ids.items():\n",
    "        identifier = v\n",
    "        metadata = []\n",
    "        \n",
    "\n",
    "#ALTERNATIVE TITLE\n",
    "       \n",
    "        alternativeTitle = \"\"\n",
    "        try:\n",
    "            alternativeTitle = str(cleanData(newdata[\"dataset\"][y]['title']))\n",
    "        except:\n",
    "            alternativeTitle = str(newdata[\"dataset\"][y]['title'])\n",
    "            \n",
    "# TITLE\n",
    "            \n",
    "        # call the format_title function\n",
    "        title = format_title(alternativeTitle, titleSource)\n",
    "\n",
    "            \n",
    "#DESCRIPTION\n",
    "\n",
    "        description = cleanData(newdata[\"dataset\"][y]['description'])\n",
    "        description = description.replace(\"{{default.description}}\", \"\").replace(\"{{description}}\", \"\")\n",
    "        description = re.sub(r'[\\n]+|[\\r\\n]+', ' ', description, flags=re.S)\n",
    "        description = re.sub(r'\\s{2,}', ' ', description)\n",
    "        description = description.translate({8217: \"'\", 8220: '\"', 8221: '\"', 160: \"\", 183: \"\", 8226: \"\", 8211: \"-\", 8203: \"\"})\n",
    "\n",
    "\n",
    "# RESOURCE TYPE\n",
    "\n",
    "        # if 'LiDAR' exists in Title or Description, add it to Resource Type\n",
    "        if 'LiDAR' in title or 'LiDAR' in description:\n",
    "            resourceType = 'LiDAR'\n",
    "                            \n",
    "#CREATOR\n",
    "        creator = newdata[\"dataset\"][y][\"publisher\"]\n",
    "        for pub in creator.values():\n",
    "            try:\n",
    "                creator = pub.replace(u\"\\u2019\", \"'\")\n",
    "            except:\n",
    "                creator = pub\n",
    "\n",
    "\n",
    "# DISTRIBUTION\n",
    "\n",
    "        information = cleanData(newdata[\"dataset\"][y]['landingPage'])\n",
    "\n",
    "        format_types = []\n",
    "        resourceClass = \"\"\n",
    "        formatElement = \"\"\n",
    "        downloadURL = \"\"\n",
    "        resourceType = \"\"\n",
    "        webService = \"\"\n",
    "        featureServer = \"\"\n",
    "        mapServer = \"\"\n",
    "        imageServer = \"\"\n",
    "\n",
    "\n",
    "\n",
    "        distribution = newdata[\"dataset\"][y][\"distribution\"]\n",
    "        for dictionary in distribution:\n",
    "            try:\n",
    "                # If one of the distributions is a shapefile, change genre/format and get the downloadURL\n",
    "                format_types.append(dictionary[\"title\"])\n",
    "                if dictionary[\"title\"] == \"Shapefile\":\n",
    "                    resourceClass = \"Datasets|Web services\"\n",
    "                    formatElement = \"Shapefile\"\n",
    "                    if 'downloadURL' in dictionary.keys():\n",
    "                        downloadURL = dictionary[\"downloadURL\"].split('?')[0]\n",
    "                    else:\n",
    "                        downloadURL = dictionary[\"accessURL\"].split('?')[0]\n",
    "\n",
    "                    resourceType = \"Vector data\"\n",
    "\n",
    "                # If the Rest API is based on an ImageServer, change genre, type, and format to relate to imagery\n",
    "                if dictionary[\"title\"] == \"ArcGIS GeoService\":\n",
    "                    if 'accessURL' in dictionary.keys():\n",
    "                        webService = dictionary['accessURL']\n",
    "\n",
    "                        if webService.rsplit('/', 1)[-1] == 'ImageServer':\n",
    "                            resourceClass = \"Imagery|Web services\"\n",
    "                            formatElement = 'Imagery'\n",
    "                            resourceType = \"Satellite imagery\"\n",
    "                    else:\n",
    "                        resourceClass = \"\"\n",
    "                        formatElement = \"\"\n",
    "                        downloadURL = \"\"\n",
    "\n",
    "            # If the distribution section of the metadata is not structured in a typical way\n",
    "            except:\n",
    "                resourceClass = \"\"\n",
    "                formatElement = \"\"\n",
    "                downloadURL = \"\"\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            if \"FeatureServer\" in webService:\n",
    "                featureServer = webService\n",
    "            if \"MapServer\" in webService:\n",
    "                mapServer = webService\n",
    "            if \"ImageServer\" in webService:\n",
    "                imageServer = webService\n",
    "        except:\n",
    "            print(identifier)\n",
    "\n",
    "\n",
    "\n",
    "# BOUNDING BOX\n",
    "        \n",
    "        try:\n",
    "            scanned_bbox = newdata[\"dataset\"][y][\"spatial\"]\n",
    "            coordinates = scanned_bbox.split(',')\n",
    "            rounded_coordinates = [str(round(float(coord), 2)) for coord in coordinates]\n",
    "            bbox = ','.join(rounded_coordinates)\n",
    "        except:\n",
    "            bbox = \"\"\n",
    "            \n",
    "# KEYWORDS\n",
    "\n",
    "        keyword = newdata[\"dataset\"][y][\"keyword\"]\n",
    "        keyword_list = []\n",
    "        keyword_list = '|'.join(keyword).replace(' ', '')\n",
    "        \n",
    "        \n",
    "# DATES\n",
    "\n",
    "        dateIssued = cleanData(newdata[\"dataset\"][y]['issued']).split('T', 1)[0] \n",
    "        temporalCoverage = \"\"\n",
    "        dateRange = \"\"\n",
    "\n",
    "        # auto-generate Temporal Coverage and Date Range\n",
    "        if re.search(r\"\\{(.*?)\\}\", title):     # if title has {YYYY} or {YYYY-YYYY}\n",
    "            temporalCoverage = re.search(r\"\\{(.*?)\\}\", title).group(1)\n",
    "            dateRange = temporalCoverage[:4] + '-' + temporalCoverage[-4:]\n",
    "        else:\n",
    "            temporalCoverage = 'Continually updated resource'\n",
    "        \n",
    "#RIGHTS\n",
    "\n",
    "        rights = cleanData(newdata[\"dataset\"][y]['license']) if 'license' in newdata[\"dataset\"][y] else \"\"\n",
    "\n",
    "\n",
    "# IDENTIFIER\n",
    "        slug = identifier.split('=', 1)[-1].replace(\"&sublayer=\", \"_\")\n",
    "        querystring = parse_qs(urlparse(identifier).query)\n",
    "        identifier_new = \"https://hub.arcgis.com/datasets/\" + \"\" + querystring[\"id\"][0]\n",
    "\n",
    "            \n",
    "# Define full metadata list\n",
    "\n",
    "        metadataList = [\n",
    "            title, \n",
    "            alternativeTitle, \n",
    "            description,\n",
    "            language,\n",
    "            displayNote,\n",
    "            creator,\n",
    "            titleSource,\n",
    "            resourceClass, \n",
    "            resourceType,\n",
    "            keyword_list, \n",
    "            dateIssued, \n",
    "            temporalCoverage,\n",
    "            dateRange, \n",
    "            spatialCoverage, \n",
    "            bbox,\n",
    "            formatElement, \n",
    "            information, \n",
    "            downloadURL, \n",
    "            mapServer, \n",
    "            featureServer,\n",
    "            imageServer, \n",
    "            slug, \n",
    "            identifier_new, \n",
    "            provider, \n",
    "            hubCode, \n",
    "            memberOf, \n",
    "            isPartOf, \n",
    "            rights,\n",
    "            accrualMethod,\n",
    "            dateAccessioned, \n",
    "            accessRights\n",
    "        ]     \n",
    "\n",
    "        # deletes items where the resourceClass is empty\n",
    "        for i in range(len(metadataList)):\n",
    "            if metadataList[6] != \"\":\n",
    "                metadata.append(metadataList[i])\n",
    "\n",
    "        newItemDict[slug] = metadata\n",
    "\n",
    "        for k in list(newItemDict.keys()):\n",
    "            if not newItemDict[k]:\n",
    "                del newItemDict[k]\n",
    "\n",
    "    return newItemDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef627b2b",
   "metadata": {},
   "source": [
    "## Run the executable code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d9c0be",
   "metadata": {},
   "source": [
    "**Declare a list to hold the scanned metadata**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2ba8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "allRecords = []\n",
    "json_ids = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1069f4",
   "metadata": {},
   "source": [
    "**Scan the metadata for each Hub**\n",
    "\n",
    "This code reads data from `hubFile.csv` using the `csv.DictReader` function. It then iterates over each row in the file and extracts values from specific columns to be used later in the script.\n",
    "\n",
    "For each row, the script also defines default values for a set of metadata fields. It then checks if the URL provided in the CSV file exists and is a valid JSON response. If the response is not valid, the script prints an error message and continues to the next row. Otherwise, it extracts dataset identifiers from the JSON response and passes the response along with the identifiers to a function called metadataNewItems. The metadata for each row is then appended to a list called `allRecords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a14451b0-cba2-46f2-87ba-967e09191ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scanning  05c-01 https://opendata.minneapolismn.gov/api/feed/dcat-us/1.1.json\n",
      "scanning  05c-02 https://information.stpaul.gov/api/feed/dcat-us/1.1.json\n"
     ]
    }
   ],
   "source": [
    "with open(hubFile, newline='', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        # Read in values from arcHubs.csv to be used within the script or as part of the metadata report\n",
    "        hubCode = row['ID']\n",
    "        url = row['Identifier']\n",
    "        provider = row['Title']\n",
    "        titleSource = row['Publisher']\n",
    "        spatialCoverage = row['Spatial Coverage']\n",
    "        isPartOf = row['ID']\n",
    "        memberOf = row['Member Of']\n",
    "        \n",
    "        # Define default values for each record\n",
    "        accrualMethod = \"ArcGIS Hub\"\n",
    "        dateAccessioned = time.strftime('%Y-%m-%d')\n",
    "        accessRights = \"Public\"\n",
    "        language = \"eng\"\n",
    "        displayNote = \"This dataset was automatically cataloged from the provider's ArcGIS Hub. In some cases, information shown here may be incorrect or out-of-date. Click the 'Visit Source' button to search for items on the original provider's website.\"\n",
    "        print(\"scanning \", hubCode, url)\n",
    "        \n",
    "        \n",
    "        response = urllib.request.urlopen(url)\n",
    "        # check if the Hub's URL is broken\n",
    "        if response.headers['content-type'] != 'application/json; charset=utf-8':\n",
    "            print(\"\\n--------------------- Data hub URL does not exist --------------------\\n\",\n",
    "                  hubCode, url,  \"\\n--------------------------------------------------------------------------\\n\")\n",
    "            continue\n",
    "        else:\n",
    "            newdata = json.load(response)\n",
    "\n",
    "\n",
    "        # Makes a list of dataset identifiers\n",
    "        newjson_ids = getIdentifiers(newdata)\n",
    "\n",
    "\n",
    "        allRecords.append(metadataNewItems(newdata, newjson_ids))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec019d73",
   "metadata": {},
   "source": [
    "**Write the scanned metadata to a CSV in the GeoBTAA Metadata Profile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6198098b-05b0-4090-ad66-546c3872f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "newItemsReport = f\"{directory}/{ActionDate}_scannedRecords.csv\"\n",
    "printItemReport(newItemsReport, fieldnames, allRecords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342edef9",
   "metadata": {},
   "source": [
    "**Remove Download links with \"tif.zip\"**\n",
    "\n",
    "A large number of items in the Download column are causing errors in the Geoportal. These items can be identified by the string \"tif.zip\". The following code removes Download links for those items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05237bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataframe from the file\n",
    "df_newitems = pd.read_csv(newItemsReport)\n",
    "\n",
    "# Replace missing values with an empty string\n",
    "df_newitems['Download'] = df_newitems['Download'].fillna('')\n",
    "\n",
    "# Remove \"tif.zip\" values in the \"Download\" column\n",
    "df_newitems.loc[df_newitems['Download'].str.contains('tif.zip', na=False), 'Download'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7cf85",
   "metadata": {},
   "source": [
    "**Drop items**\n",
    "\n",
    "Duplicate IDs: ArcGIS Hub administrators can include datasets from other Hubs in their own site. As a result, some datasets are duplicated in other Hubs. However, they always have the same Identifier, so we can use pandas to detect and remove duplicate rows.\n",
    "\n",
    "Duplicate titles: Some organizations offer web services of the same data with different styling. This can result in dozens of nearly identical layers with the same title. To minimize redundant records, we will delete items with duplicate titles.\n",
    "\n",
    "Missing resource: By this point in the script, there will still be items that have note been identified as a dataset or imagery. These are typically external links to websites or PDF maps.  We drop these items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39ce2446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates based on the 'ID' column\n",
    "df_newitems = df_newitems.drop_duplicates(subset=['ID'])\n",
    "\n",
    "# Drop duplicates based on the 'Title' column\n",
    "df_newitems = df_newitems.drop_duplicates(subset=['Title'])\n",
    "\n",
    "# Drop items without a Resource Class, as they are likely web apps or text documents\n",
    "df_newitems = df_newitems.dropna(subset=['Resource Class'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d29203",
   "metadata": {},
   "source": [
    "## Print to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29873b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the modified dataframe back to the file\n",
    "df_newitems.to_csv(newItemsReport, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
